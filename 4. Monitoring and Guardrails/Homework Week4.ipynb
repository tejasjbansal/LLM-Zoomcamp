{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e925cf-3a06-45a1-b4c2-7d5c64800573",
   "metadata": {},
   "source": [
    "## Q1. Getting the embeddings model\n",
    "\n",
    "Now, get the embeddings model multi-qa-mpnet-base-dot-v1 from the Sentence Transformer library\n",
    "\n",
    "Create the embeddings for the first LLM answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c091c70-42e3-412f-ae98-44ea7a081eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f88b4e1-e11a-4887-85c0-4deb2b2e73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://github.com/tejasjbansal/LLM-Zoomcamp/blob/main'\n",
    "relative_url = '4.%20Monitoring%20and%20Guardrails/data/results-gpt4o-mini.csv'\n",
    "url = f'{base_url}/{relative_url}?raw=1'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b10c9d-6fba-435f-aa2f-cb9df2c63d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03748d12-46fc-48e0-aa04-127dcac97aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_llm = df.iloc[0].answer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b2e7cf-8760-4bd8-be60-0b6d79b5ef8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42244655"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.encode(answer_llm)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d4bd8-05f1-406a-b930-4deb2037680b",
   "metadata": {},
   "source": [
    "## Q2. Computing the dot product\n",
    "\n",
    "Now for each answer pair, let's create embeddings and compute dot product between them\n",
    "\n",
    "We will put the results (scores) into the evaluations list\n",
    "\n",
    "What's the 75% percentile of the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a5dfb7d-0b3c-44f5-b85d-d88d51ed07fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [01:24,  3.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for row in tqdm(df[['answer_llm', 'answer_orig']].itertuples(index=False)):\n",
    "    v_llm = embedding_model.encode(row.answer_llm)\n",
    "    v_orig = embedding_model.encode(row.answer_orig)\n",
    "\n",
    "    evaluations.append(v_llm.dot(v_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d91813d-274d-419c-a224-84f951283a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['evaluations'] = evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43f04836-2ae1-4805-9089-b20f5ed76856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean      27.495996\n",
       "std        6.384742\n",
       "min        4.547923\n",
       "25%       24.307844\n",
       "50%       28.336870\n",
       "75%       31.674309\n",
       "max       39.476013\n",
       "Name: evaluations, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['evaluations'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f39ece8-b5cb-4931-b7d8-434b7f8752ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.515987"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c95dfa9-1e85-457b-928a-e93e8e8f1dc3",
   "metadata": {},
   "source": [
    "## Q3. Computing the cosine\n",
    "\n",
    "From Q2, we can see that the results are not within the [0, 1] range. It's because the vectors coming from this model are not normalized.\n",
    "\n",
    "So we need to normalize them.\n",
    "\n",
    "To do it, we\n",
    "\n",
    "- Compute the norm of a vector\n",
    "- Divide each element by this norm\n",
    "\n",
    "So, for vector v, it'll be v / ||v||\n",
    "\n",
    "In numpy, this is how you do it:\n",
    "\n",
    "- `norm = np.sqrt((v * v).sum())`\n",
    "- `v_norm = v / norm`\n",
    "\n",
    "Let's put it into a function and then compute dot product between normalized vectors. This will give us cosine similarity\n",
    "\n",
    "What's the 75% cosine in the scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e319913-b3ad-4ce5-944e-347112cc8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gpt4o = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e42911b2-e66e-4d9e-9305-a19a28ebd643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = embedding_model.encode(answer_llm)\n",
    "    v_orig = embedding_model.encode(answer_orig)\n",
    "\n",
    "    norm = np.sqrt((v_llm * v_llm).sum())\n",
    "    v_norm_llm = v_llm / norm\n",
    "\n",
    "    norm = np.sqrt((v_orig * v_orig).sum())\n",
    "    v_norm_orig = v_orig / norm\n",
    "    \n",
    "    return v_norm_llm.dot(v_norm_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0b2b032-d9c8-4c56-b261-08cef213ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:22<00:00,  3.62it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(results_gpt4o):\n",
    "    sim = compute_similarity(record)\n",
    "    evaluations.append(sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8d41d80-ea05-40e3-bd29-891ac50dce95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean       0.728393\n",
       "std        0.157755\n",
       "min        0.125357\n",
       "25%        0.651273\n",
       "50%        0.763761\n",
       "75%        0.836235\n",
       "max        0.958796\n",
       "Name: evaluations, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['evaluations'] = evaluations\n",
    "df['evaluations'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359bae6-8c9a-4ce3-a0c9-cdc39177f5e3",
   "metadata": {},
   "source": [
    "## Q4. Rouge\n",
    "\n",
    "Now we will explore an alternative metric - the ROUGE score.\n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "We don't need to implement it ourselves, there's a python package for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16259b83-8f40-4e3f-a184-5568d9044505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.10/site-packages (from rouge) (1.16.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84d03c-02f4-4112-9711-a6baa36930e7",
   "metadata": {},
   "source": [
    "(The latest version at the moment of writing is 1.0.1)\n",
    "\n",
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (doc_id=5170565b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7073f182-7408-455d-9466-d4d113186d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df[df['document']=='5170565b'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b1e707c2-3a9c-4145-b1f7-2cfbdd44ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29df2699-54f3-426b-b8e3-e42bbfdcaa67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.45454545454545453,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.45454544954545456},\n",
       " 'rouge-2': {'r': 0.21621621621621623,\n",
       "  'p': 0.21621621621621623,\n",
       "  'f': 0.21621621121621637},\n",
       " 'rouge-l': {'r': 0.3939393939393939,\n",
       "  'p': 0.3939393939393939,\n",
       "  'f': 0.393939388939394}}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90439a00-cdca-443b-a2cf-55453efae1ca",
   "metadata": {},
   "source": [
    "## Q5. Average rouge score\n",
    "\n",
    "Let's compute the average between rouge-1, rouge-2 and rouge-l for the same record from Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b61490-4268-4060-bb90-295a69b4dbec",
   "metadata": {},
   "source": [
    "To compute the average of the ROUGE-1, ROUGE-2, and ROUGE-L scores for recall, precision, and F-measure, we will take the arithmetic mean of the corresponding values from each metric.\n",
    "\n",
    "Here are the given values:\n",
    "- **ROUGE-1:** \\( r = 0.4545 \\), \\( p = 0.4545 \\), \\( f = 0.4545 \\)\n",
    "- **ROUGE-2:** \\( r = 0.2162 \\), \\( p = 0.2162 \\), \\( f = 0.2162 \\)\n",
    "- **ROUGE-L:** \\( r = 0.3939 \\), \\( p = 0.3939 \\), \\( f = 0.3939 \\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "65a7dba1-6996-4289-b562-d2d2f5079c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35490035490035493, 0.35490035490035493, 0.35490034990035496)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the recall, precision, and F-measure values for each ROUGE metric\n",
    "rouge_1 = {'r': 0.45454545454545453, 'p': 0.45454545454545453, 'f': 0.45454544954545456}\n",
    "rouge_2 = {'r': 0.21621621621621623, 'p': 0.21621621621621623, 'f': 0.21621621121621637}\n",
    "rouge_l = {'r': 0.3939393939393939, 'p': 0.3939393939393939, 'f': 0.393939388939394}\n",
    "\n",
    "# Compute the averages\n",
    "average_recall = (rouge_1['r'] + rouge_2['r'] + rouge_l['r']) / 3\n",
    "average_precision = (rouge_1['p'] + rouge_2['p'] + rouge_l['p']) / 3\n",
    "average_fmeasure = (rouge_1['f'] + rouge_2['f'] + rouge_l['f']) / 3\n",
    "\n",
    "average_recall, average_precision, average_fmeasure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b86951-f198-426d-a997-ff8568eacf50",
   "metadata": {},
   "source": [
    "## Q6. Average rouge score for all the data points\n",
    "\n",
    "Now let's compute the score for all the records\n",
    "\n",
    "```rouge_1 = scores['rouge-1']['f']\n",
    "rouge_2 = scores['rouge-2']['f']\n",
    "rouge_l = scores['rouge-l']['f']\n",
    "rouge_avg = (rouge_1 + rouge_2 + rouge_l) / 3\n",
    "\n",
    "And create a dataframe from them\n",
    "\n",
    "What's the agerage rouge_l across all the records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "97f1a7c5-eab2-4d11-8cb8-3529361b1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_score(r):\n",
    "    scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cd442e2b-e8e7-4455-bc0e-85c5facb7975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 362.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>rouge-avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.072882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.091435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.389610</td>\n",
       "      <td>0.327658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.150821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.142076</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.120219</td>\n",
       "      <td>0.098731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.604570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.460432</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.535991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.654867</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>0.618851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.304762</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>0.247252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.118954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rouge-1   rouge-2   rouge-l  rouge-avg\n",
       "0    0.095238  0.028169  0.095238   0.072882\n",
       "1    0.125000  0.055556  0.093750   0.091435\n",
       "2    0.415584  0.177778  0.389610   0.327658\n",
       "3    0.216216  0.047059  0.189189   0.150821\n",
       "4    0.142076  0.033898  0.120219   0.098731\n",
       "..        ...       ...       ...        ...\n",
       "295  0.654545  0.540984  0.618182   0.604570\n",
       "296  0.590164  0.460432  0.557377   0.535991\n",
       "297  0.654867  0.564516  0.637168   0.618851\n",
       "298  0.304762  0.132231  0.304762   0.247252\n",
       "299  0.179487  0.023529  0.153846   0.118954\n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for record in tqdm(results_gpt4o):\n",
    "    sim = rouge_score(record)\n",
    "    rouge_1 = sim['rouge-1']['f']\n",
    "    rouge_2 = sim['rouge-2']['f']\n",
    "    rouge_l = sim['rouge-l']['f']\n",
    "    rouge_avg = (rouge_1 + rouge_2 + rouge_l) / 3\n",
    "\n",
    "    data.append({\n",
    "            'rouge-1': rouge_1,\n",
    "            'rouge-2': rouge_2,\n",
    "            'rouge-l': rouge_l,\n",
    "            'rouge-avg': rouge_avg\n",
    "        })\n",
    "    \n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "74d5ffa3-078f-4e8e-b2f9-3803c1b740a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3538074656078652"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rouge-l'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
